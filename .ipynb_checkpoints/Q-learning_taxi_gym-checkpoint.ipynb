{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9HQWDhO3P4dn"
   },
   "source": [
    "# Learning Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ImHmchN_I2d1"
   },
   "source": [
    "I'm learning reinforcement learning from the following. I think I will say reinforcement learning is a training to get the best action in each state in the state space. But I'm not clear about when a taxi run into a wall and evaluation. <br> \n",
    "https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVLLklarQrtr"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KuuxjS_IM9R"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# reinforcement learning examples in it\n",
    "import gym\n",
    "\n",
    "# visualize training\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# use sleep to get a snapshot and to visualize training process\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5idfLqiwWN-0"
   },
   "source": [
    "## Self-driving cab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1271,
     "status": "ok",
     "timestamp": 1571680434389,
     "user": {
      "displayName": "Yuki Kitayama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64",
      "userId": "10996233055866244291"
     },
     "user_tz": 240
    },
    "id": "Ox-NYrLaIVIo",
    "outputId": "7c9a40d1-7f3b-4a60-b648-26ecc8a49549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# taxi example of reinforcement learning\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "\n",
    "# fix the initial location of a taxi\n",
    "env.s = env.encode(3, 4, 2, 0)\n",
    "# vertical and horizontal states start from 0, so 0, 1, 2, 3, 4 for each\n",
    "\n",
    "# show the taxi and a park\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwKU935SIjvk"
   },
   "source": [
    "Box is a taxi. The initial location of a taxi randomly will be assigned. Yellow means the taxi is empty. Once a passanger takes it, the box becomes black. Alphabets are possible locations for pickup or destination. The blue shows a passener is there, and the pink is the place he wants to go. After Q-learning, we will see what the best action is in this location. Since a passenger is at Y, a taxi should go north or west first step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1570303476608,
     "user": {
      "displayName": "Yuki Kitayama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64",
      "userId": "10996233055866244291"
     },
     "user_tz": 240
    },
    "id": "WcyPoY7zIqJp",
    "outputId": "d9d123cd-e69e-436e-ac83-bdb001eb22ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n",
      "Discrete(500)\n",
      "388\n",
      "{0: [(1.0, 488, -1, False)], 1: [(1.0, 288, -1, False)], 2: [(1.0, 388, -1, False)], 3: [(1.0, 368, -1, False)], 4: [(1.0, 388, -10, False)], 5: [(1.0, 388, -10, False)]}\n"
     ]
    }
   ],
   "source": [
    "# show the number of action states\n",
    "print(env.action_space)\n",
    "\n",
    "# show the number of observation state, which is 5 * 5 * 4 * (1 + 4) \n",
    "# (Vetical * horizontal * pickup location * passenger-in-the-taxi state and destination)\n",
    "print(env.observation_space)\n",
    "\n",
    "# internal logic\n",
    "print(env.s)\n",
    "print(env.P[env.s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJe1V7EWNTQO"
   },
   "source": [
    "env.P is {action: [(probability, next state, reward, done)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5nZoY_OW3li"
   },
   "source": [
    "## Reinforcement learning - Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1570298775629,
     "user": {
      "displayName": "Yuki Kitayama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64",
      "userId": "10996233055866244291"
     },
     "user_tz": 240
    },
    "id": "CXVQNVAAcTVl",
    "outputId": "1baa8c2c-5253-47f6-f6d5-3871e530835e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n"
     ]
    }
   ],
   "source": [
    "# initialize state action space matrix to store learned values later\n",
    "\n",
    "# possibilities for all the state spaces and action spaces\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "print(q_table.shape)\n",
    "\n",
    "# action states = {south, north, east, west, pickup, dropoff}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1570298823845,
     "user": {
      "displayName": "Yuki Kitayama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64",
      "userId": "10996233055866244291"
     },
     "user_tz": 240
    },
    "id": "yvEvCuVkdG44",
    "outputId": "709b123c-a245-40cb-929b-1056e3705f12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 44.3 s, sys: 2.81 s, total: 47.2 s\n",
      "Wall time: 45.9 s\n"
     ]
    }
   ],
   "source": [
    "# show training time\n",
    "%%time\n",
    "\n",
    "# hyperparameters for q learning\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "# use grid search to know these values if you want to\n",
    "\n",
    "for i in range(1, 100001):\n",
    "  \n",
    "  # initialize by reset\n",
    "  state = env.reset()\n",
    "  epochs = 0\n",
    "  penalties = 0\n",
    "  reward = 0\n",
    "  done = False\n",
    "  \n",
    "  # done is successful passenger dropoff\n",
    "  while not done:\n",
    "    \n",
    "    # using epsilon logic deliberately avoid taking best route many time\n",
    "    # it can reduce overfitting.\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "      # explore action space\n",
    "      action = env.action_space.sample()\n",
    "      # deliberately sample action states from 0,1,2,3,4,5 to explore new possibilities\n",
    "      \n",
    "    else:\n",
    "      # exploit learned values\n",
    "      action = np.argmax(q_table[state])\n",
    "      # draw best action\n",
    "      \n",
    "    # draw following info from the action we decided\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # draw q value in a certain state and with a certain action\n",
    "    old_value = q_table[state, action]\n",
    "    \n",
    "    # we already took action, so we have next state\n",
    "    # np.max draw the maximum q value from each action in a certain state\n",
    "    next_max = np.max(q_table[next_state])\n",
    "    \n",
    "    # The most important q value update algorithm for Q-learning\n",
    "    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "    # It's a combination of reward of current action in current state \n",
    "    # and discounted maximum reward from next state from current action\n",
    "    \n",
    "    # update Q-table of a certain state of a certain action\n",
    "    q_table[state, action] = new_value \n",
    "    \n",
    "    # if making a big mistake, accumulate penalty\n",
    "    if reward == -10:\n",
    "      penalties += 1\n",
    "     \n",
    "    # for iteration\n",
    "    state = next_state\n",
    "    epochs += 1\n",
    "    \n",
    "  if i % 100 == 0:\n",
    "    clear_output(wait = True)\n",
    "    print(f\"Episode: {i}\")\n",
    "    \n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PCKp9e0SY64s"
   },
   "source": [
    "## Check our initial intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1570303485332,
     "user": {
      "displayName": "Yuki Kitayama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64",
      "userId": "10996233055866244291"
     },
     "user_tz": 240
    },
    "id": "2yItWZ_KGFYe",
    "outputId": "06b08fe8-b456-496d-f081-80d8002984f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.4737387 , -2.4510224 , -2.4594717 , -2.4510224 , -9.6392277 ,\n",
       "       -9.64972901])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[388]\n",
    "# largest value action state is north"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PV8PPCggY-Md"
   },
   "source": [
    "The highest values in Q table is the second and forth one, which are north and west, so we got the right answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHXzrGcph0tr"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1570299791536,
     "user": {
      "displayName": "Yuki Kitayama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64",
      "userId": "10996233055866244291"
     },
     "user_tz": 240
    },
    "id": "bgQhCzbDI8AS",
    "outputId": "d0df6bc5-4d7a-49db-98f7-d5d9cee049aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "12.07\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "total_epochs = 0\n",
    "total_penalties = 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "  state = env.reset()\n",
    "  epochs = 0\n",
    "  penalties = 0\n",
    "  reward = 0\n",
    "  done = False\n",
    "  \n",
    "  while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    if reward == -10:\n",
    "      penalties += 1\n",
    "      \n",
    "    epochs += 1\n",
    "    \n",
    "  total_penalties += penalties\n",
    "  total_epochs += epochs\n",
    "  \n",
    "print(episodes)\n",
    "print(total_epochs / episodes)\n",
    "print(total_penalties / episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyapGnFFXwwL"
   },
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1570298491749,
     "user": {
      "displayName": "Yuki Kitayama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64",
      "userId": "10996233055866244291"
     },
     "user_tz": 240
    },
    "id": "SX_t2QpqPlAG",
    "outputId": "0e411cfd-06fc-4c50-a970-5893d0bac084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "# non-reinforcement learning example\n",
    "env.s = 328\n",
    "epochs = 0\n",
    "penalties = 0\n",
    "reward = 0\n",
    "frames = []\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "  action = env.action_space.sample()\n",
    "  state, reward, done, info = env.step(action)\n",
    "  \n",
    "  if reward == -10:\n",
    "    penalties += 1\n",
    "    \n",
    "  frames.append({'frame': env.render(mode = 'ansi'),\n",
    "                 'state': state,\n",
    "                 'action': action,\n",
    "                 'reward': reward})\n",
    "  \n",
    "  epochs += 1\n",
    "\n",
    "print(epochs)\n",
    "print(penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1570284920630,
     "user": {
      "displayName": "Yuki Kitayama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64",
      "userId": "10996233055866244291"
     },
     "user_tz": 240
    },
    "id": "i97Vp31NQndo",
    "outputId": "f507e215-3e9d-4817-91ac-88e937054627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 296\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "# visualize moving process\n",
    "def print_frames(frames):\n",
    "  for i, frame in enumerate(frames):\n",
    "    clear_output(wait = True)\n",
    "    print(frame['frame'].getvalue())\n",
    "    print(f\"Timestep: {i + 1}\")\n",
    "    print(f\"State: {frame['state']}\")\n",
    "    print(f\"Action: {frame['action']}\")\n",
    "    print(f\"Reward: {frame['reward']}\")\n",
    "    # to show a snapshop, delay the next image update by sleep function from time module\n",
    "    sleep(.1)\n",
    "  \n",
    "# when a box becomes black, it means a taxi picks up a passenger\n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1570305523810,
     "user": {
      "displayName": "Yuki Kitayama",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAzogjSXmQEQOmdbgpJirOyfehV4O2YrYeJ7dWPAg=s64",
      "userId": "10996233055866244291"
     },
     "user_tz": 240
    },
    "id": "F0d_auRMfLiR",
    "outputId": "fce0cf53-0de7-460b-ced9-c8c088b8a4ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n",
      "3\n",
      "(468, -1, False, {'prob': 1.0})\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "action = env.action_space.sample()\n",
    "print(action) \n",
    "\n",
    "# next_state, reward, done, info = env.step(action)\n",
    "print(env.step(action))\n",
    "\n",
    "print(env.reset())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Learning_Reinforcement_Learning_OpenAI_Gym.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
